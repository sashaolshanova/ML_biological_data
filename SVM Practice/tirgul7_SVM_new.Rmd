---
title: "Practice 9 - SVM"
author: "Romi Goldner Kabeli"
date: "6/05/2022"
output: html_document
---

# Heart Disease Prediction with SVM

Today we will use SVM on the Heart Disease data to predict whether a patient is suffering from any heart disease or not.

```{r setup}
set.seed(10)

library(tidyverse)    # data manipulation and visualization
library(caret)        # SVM methodology
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(pROC)
library(RColorBrewer) # customized coloring of plots
library(corrplot)
library(ggplot2)
```

As always, we start by reading and looking at the data.

```{r}
# read and look at data
heart_data <- read.csv("heart.csv")
str(heart_data)
```
We see that the data consists of various attributes like the person’s age, sex, cholesterol level and etc. 
We will try to predict the target (heart disease).

```{r}
# check for NA's
anyNA(heart_data)
```

```{r}
# look at summary of the data
summary(heart_data)
```

We see that all data is numeric, but some of these values could be replaced with more meaningful labels.
Before we start the classification, lets change the data to be as such. 

```{r}
heart_data_edit <- heart_data %>% 
        mutate(sex = if_else(sex == 1,"MALE","FEMALE"),
              fbs = if_else (fbs ==1 ,">120", "<=120"),
              exang = if_else (exang ==1 ,"YES", "NO"),
               cp = if_else (cp == 1, "ATYPICAL ANGINA",
                            if_else(cp == 2, "NON-ANGINAL PAIN","ASYMPTOMATIC")),
               restecg = if_else(restecg == 0, "NORMAL",
                                if_else(restecg == 1, "ABNORMALITY", "PROBABLE OR DEFINITE")),
               slope = as.factor(slope),
               ca = as.factor(ca),
               thal = as.factor(thal),
               target = if_else(target ==1 ,"YES","NO")) %>%
    mutate_if(is.character, as.factor)%>%
    dplyr::select(target,sex,fbs,exang,cp,restecg,slope,ca,thal,everything())
```

```{r}
summary(heart_data_edit)
```

### Data Exploration

We will explore the data a bit.

```{r}
ggplot(heart_data_edit, aes(x=heart_data_edit$target, fill=heart_data_edit$target)) + 
    geom_bar()+
    xlab("Heart Disease")+
    ylab("count")+
    ggtitle("Presence & Absence of Heart disease")+
    scale_fill_discrete(name= 'Heart Disease', labels =c("Absence", "Presence"))
```

```{r}
prop.table(table(heart_data_edit$target))
```


```{r}
heart_data_edit %>% 
    group_by(age) %>%
    count() %>%
    filter(n>10) %>%
    ggplot() +
    geom_col(aes(age,n),fill = 'red')+
    ggtitle("Age Analysis")+
    xlab("Age")+
    ylab("Age Count")
```
```{r}
cor_heart <- cor(heart_data_edit[,10:14])
corrplot(cor_heart, method ='square')
```

### Train Model

We will start by splitting the data into train/test. Because of the size of this dataset we will use a 70:30 partition.

```{r}
# split data into train and test sets
intrain <- createDataPartition(y = heart_data_edit$target, p= 0.7, list = FALSE)
heart_train <- heart_data_edit[intrain,]
heart_test <- heart_data_edit[-intrain,]
```

Now we can built the actual model:

```{r}
# Control the training. Used to specify repeated K-fold cross-validation (and the argument repeats controls the number of repetitions). 
# K is controlled by the number argument and defaults to 10. 
# classProbs is used to enable ROC calculation.
fitControl <- trainControl(method = "repeatedcv", 
                           repeats = 5,
                           classProbs = TRUE)
```

```{r}
set.seed(10)
svmModel <- train(target ~ ., data = heart_train,
                  method = "svmRadial", 
                  trControl = fitControl ,
                  preProcess = c("center", "scale"),
                  tunelength = 8,
                  metric = `Balanced Accuracy` )
svmPrediction <- predict(svmModel, heart_test)
svmPredictionprob <- predict(svmModel, heart_test, type='prob')[2]
svmConfMat <- confusionMatrix(svmPrediction, heart_test[,"target"])
```

```{r}
# ROC curve
roc(as.numeric(heart_test$target),as.numeric(as.matrix((svmPredictionprob))))$auc
```

```{r}
# SVM Confusion Matrix
svmConfMat
```

Here we used the svmRadial method. There are other methods for svm that can be used that could improve the preformance. 
Other kernals might prove even better, or the cost of constraints parameter C could be varied to modify the width of the decision boundary. 


### Improving SVM Model

Let's try using a different method.

```{r}
library(e1071)
library(caret)

# Set the seed for reproducibility
set.seed(10)

# Define the training control
fitControl <- trainControl(
  method = "repeatedcv",
  repeats = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary  # Use ROC as the summary function
)

# Perform feature scaling and centering as part of the preprocessing step
preprocess <- c("center", "scale")

# Train the SVM model using "svmLinear" method (different kernel method)
svmModel <- train(
  target ~ .,
  data = heart_train,
  method = "svmLinear",
  trControl = fitControl,
  preProcess = preprocess,
  metric = "ROC"
)
```

```{r}
# Make predictions on the test set
svmPrediction <- predict(svmModel, heart_test)
svmPredictionProb <- predict(svmModel, heart_test, type = "prob")[, 2]

# Evaluate the model performance
svmConfMat <- confusionMatrix(svmPrediction, heart_test[,"target"])

# Print the accuracy and confusion matrix
svmAccuracy <- svmConfMat$overall["Accuracy"]
print(paste("Accuracy:", svmAccuracy))
print("Confusion Matrix:")
print(svmConfMat$table)
```


We tried to improve the model but seems like the first try did well.


#### SVM compared with MMC and SVC

(This example was taken from https://afit-r.github.io/svm)

In cases where the data is linearly separable (can be easily separated with a single line), 
SVM is not required. We can simply use a Maximal Margin Classifier (MMC) to separate the data. W
When the data is not linearly separable, we can use a support vector classifier (SVC) (sub-method of SVM). 

Lets see how the three methods compare:

## MMC

```{r}
set.seed(123)
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

We can see that the data can be divided with a single line, but we can still draw an infinite number of such lines. 
MMC helps us find the best one. First, we use the e1071 package:

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit, dat)
```

The points marked as X were used to calculate the line, while the points marked with 0 were ignored. 

## SVC - Support Vector Classifiers

What about when the cases can't be easily separated?

```{r}
set.seed(457)
# Construct sample data set - not completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
dat <- data.frame(x=x, y=as.factor(y))

# Plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

Whether the data is separable or not, the svm() command syntax is the same. 
In the case of data that is not linearly separable, however, the cost argument takes on real importance. This quantifies the penalty associated with having an observation on the wrong side of the classification boundary. We can plot the fit in the same way as the completely separable case. 
We first use e1071:

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y~., data = dat, kernel = "linear", cost = 10)
# Plot Results
plot(svmfit, dat)
```

The higher the value of cost, the more the model tries to avoid a mis-classification. But how can we decide what is the "best" cost for our data? Instead of specifying a cost up front, we can use the tune() function from e1071 to test various costs and identify which value produces the best fitting model:

```{r}
# find optimal cost of mis-classification
tune.out <- tune(svm, y~., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# extract the best model
(bestmod <- tune.out$best.model)
```

With the optimal cost calculated, we can construct a table of predicted classes against true classes using the predict() command as follows:

```{r}
ypred <- predict(bestmod, dat)
(misclass <- table(predict = ypred, truth = dat$y))
```

Using this support vector classifier did an ok job, but the data we generated was small.


## SVM

The above mentioned SVC is a specific case of SVM, which is a more robust, generalized method. The options for classification structures using the svm() command from the e1071 package are linear, polynomial, radial, and sigmoid. To demonstrate a nonlinear classification boundary, we will construct a new data set:

```{r}
# construct larger random data set
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

Notice that the data is not linearly separable, and furthermore, isn’t all clustered together in a single group. There are two sections of class 1 observations with a cluster of class 2 observations in between. To demonstrate the power of SVMs, we’ll take 100 random observations from the set and use them to construct our boundary. We set kernel = "radial" based on the shape of our data and plot the results.

```{r}
# set pseudorandom number generator
set.seed(123)
# sample training data and fit model
train <- base::sample(200,100, replace = FALSE)
svmfit <- svm(y~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
# plot classifier
plot(svmfit, dat)
```

Again, using tune to find the best cost for our data:

```{r}
# tune model to find optimal cost, gamma values
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial",
                 ranges = list(cost = c(0.1,1,10,100,1000)))
# show best model
tune.out$best.model
```

```{r}
# validate model performance
(valid <- table(true = dat[-train,"y"], pred = predict(tune.out$best.model,
                                             newx = dat[-train,])))
```

# SVM on data with more than 2 classes

```{r}
# construct data set
x <- rbind(x, matrix(rnorm(50*2), ncol = 2))
y <- c(y, rep(0,50))
x[y==0,2] <- x[y==0,2] + 2.5
dat <- data.frame(x=x, y=as.factor(y))
# plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000","#FF0000","#00BA00")) +
  theme(legend.position = "none")
```

The commands don’t change for the e1071 package. We specify a cost and tuning parameter γ and fit a support vector machine. The results and interpretation are similar to two-class classification.

```{r}
# fit model
svmfit <- svm(y~., data = dat, kernel = "radial", cost = 10, gamma = 1)
# plot results
plot(svmfit, dat)
```

We can check to see how well our model fit the data by using the predict() command, as follows:

```{r}
# construct table
ypred <- predict(svmfit, dat)
(misclass <- table(predict = ypred, truth = dat$y))
```

Using the kernlab package, to visualize the results of the ksvm function, we take the steps listed below to create a grid of points, predict the value of each point, and plot the results:

```{r}
# fit and plot
kernfit <- ksvm(as.matrix(dat[,2:1]),dat$y, type = "C-svc", kernel = 'rbfdot', 
                C = 100, scaled = c())

# Create a fine grid of the feature space
x.1 <- seq(from = min(dat$x.1), to = max(dat$x.1), length = 100)
x.2 <- seq(from = min(dat$x.2), to = max(dat$x.2), length = 100)
x.grid <- expand.grid(x.2, x.1)

# Get class predictions over grid
pred <- predict(kernfit, newdata = x.grid)

# Plot the results
cols <- brewer.pal(3, "Set1")
plot(x.grid, pch = 19, col = adjustcolor(cols[pred], alpha.f = 0.05))

classes <- matrix(pred, nrow = 100, ncol = 100)
contour(x = x.2, y = x.1, z = classes, levels = 1:3, labels = "", add = TRUE)

points(dat[, 2:1], pch = 19, col = cols[predict(kernfit)])
```

To conclude, SVM is a robust classification method that can be used on easy or difficult datasets 
to separate data with 2 or more classes.
Because it's a 'black box', it's very useful as long as you don't need to know what is 
happening behind the scenes.

