---
title: "Final project, Gram negative bacteria permeation"
author: "Dina Khasanova(768033598) and Aleksandra Olshanova (728945613)"
date: "2023-06-15"
output: pdf_document
---
For this project we will build a classification model, which predict compound's accumulation in gram-negative bacteria cells.

The data set comprises of compound names and their corresponding SMILES representations (187 compounds all together). Additionally, it includes information such as
the formal charge of each compound, permeability coefficients for developing a regression model, and accumulation classes for constructing a classification model. The data set also contains residue data obtained during the docking algorithm, which is represented by an interaction fingerprint string. Each residue is encoded with zeros and ones to denote different types of interactions, including hydrogen bonds (HAccep, HDonor, and Ar-Hbond), halogen bonds (XBond), salt-bridge interactions (Salt), pi-cation interactions (PiCat), pi-pi interactions
(PiFace, PiEdge), and hydrophobic interactions (HPhob). Each compound has from 13 to 15 conformations after docking. 

First of all, we will upload essential libraries for analysing the data, and building a model.
```{r include=FALSE}
library(dplyr)
library(ggplot2)
library(plotly)
library(glue)
library(corrplot)
library(Rtsne)
library(tidymodels)
library('fastDummies')
library(randomForest)
library(caret)
library(patchwork)
library(gridExtra)
library(e1071)
library(class)
library(sjmisc)
```
Uploading the data:
```{r}
data <- read.csv("data_final_project_cleaned.csv")
dim(data)
names(data[, 0:10])
```
After uploading the data, a crucial step involves searching for NaN values. The presence of NaN values in the interaction fingerprint indicates that the compound was successfully docked within the channel, but no specific types of interactions were detected. To change it, NaN values were substituted with "0", meaning the absence of the interaction. 
```{r}
anyNA(data)
data[is.na(data)] <- as.integer(0)
```
After applying the docking algorithm, our data set consists of 2606 poses, accompanied by 784 generated interaction features, and 6 essential general properties necessary for model construction. It is worth noting that some interaction fingerprint features may have identical values, resulting in their insignificance for data point separation. Below is a summary of these specific features.
```{r}
summary(data[, 7:10])
```
Now, we will delete these features, and explore the left ones.
```{r}
list_residues = names(data[, 7:790])

myList_delete <- c()
myList_save <- c()
for (name in list_residues) {
  sum_value = sum(data[name])
  if (sum_value == 0) {
    myList_delete <- append(myList_delete, name)
  }
  else {
    myList_save <- append(myList_save, name)
  }
}

df = data[,!(names(data) %in% myList_delete)]

glue('Number of features to save: {length(myList_save)}. 
     Number of features to delete {length(myList_delete)}.')
```
Based on the literature, we have identified 5 crucial residues that are highly favorable for interactions, leading to a significant increase in accumulation value. Notably, binding to ASP113 greatly enhances the compound's penetration capability. Subsequently, we have created lists including all found types of interactions facilitated by these 5 key residues.
![Important residues inside the channel](/Users/sashaolshanova/Git/machine learning course/FINAL_PROJECT/imp_res.png)
```{r}
asp113 <- c("A.113.ASP..HDonor.nc", "A.113.ASP..HDonor.cc",
            "A.113.ASP..HDonor.nn", "A.113.ASP..HDonor.cn", 
            "A.113.ASP..Salt", "A.113.ASP..HPhob")
glu117 <- c("A.117.GLU..HDonor.nc", "A.117.GLU..HDonor.cn", 
            "A.117.GLU..HDonor.nn", "A.117.GLU..HDonor.cc", 
            "A.117.GLU..Salt", "A.117.GLU..HPhob", "A.117.GLU..XBond")
arg132 <- c("A.132.ARG..HAccep.nn", "A.132.ARG..HAccep.cn",
            "A.132.ARG..HAccep.cc", "A.132.ARG..HAccep.nc",
            "A.132.ARG..XBond","A.132.ARG..Salt", "A.132.ARG..PiCat" )
arg82 <-  c("A.82.ARG..HAccep.nn", "A.82.ARG..HAccep.cn", 
            "A.82.ARG..HAccep.cc", "A.82.ARG..HAccep.nc", 
            "A.82.ARG..XBond","A.82.ARG..Salt","A.82.ARG..PiCat")
agr42 <-  c("A.42.ARG..HAccep.nn", "A.42.ARG..HAccep.cn",
            "A.42.ARG..HAccep.cc", "A.42.ARG..HAccep.nc","A.42.ARG..XBond",
            "A.42.ARG..Salt","A.42.ARG..PiCat", "A.42.ARG..HPhob")
lys16 <-  c("A.16.LYS..HAccep.cn", "A.16.LYS..HAccep.cc",
            "A.16.LYS..XBond", "A.16.LYS..Salt", "A.16.LYS..PiCat", "A.16.LYS..HPhob")
list_important_res <-list(list1 = asp113, list2 = glu117, 
                          list3 = arg132 , list4= arg82 , list5 = lys16)
```
Given that each compound consists of approximately 13-15 poses, we plan to employ the "Voting algorithm" for the interaction fingerprint strings. This algorithm will enable us to select a representative interaction fingerprint string from the 15 available poses, effectively describing our compound's interactions.
The "Voting algorithm" is described on the picture below.

![Voting algorithm](/Users/sashaolshanova/Git/machine learning course/FINAL_PROJECT/voting.png)
A threshold of "2" means an interaction exists in at least half of the poses to be considered in the compound. Similarly, a threshold of "15" means an interaction exists in at least one pose to be considered in the compound.
Explored thresholds: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. 

Additionally, we have established individual columns for all significant residues, utilizing binary notation with zeros and ones. In this context, a value of "1" signifies the presence of at least one interaction type, such as hydrogen bonds, halogen bonds, salt-bridge interactions, pi-cation interactions, pi-pi interactions, or hydrophobic interactions. A value of "0" indicates the absence of these interactions.

```{r}
threshold_list <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)
empty_dict <- list()

real_data = data.frame(matrix(ncol = 182, nrow = 0))
colnames(real_data) <- c(names(df))

for (number in threshold_list) {
  
  empty_dict[[paste0("df_threshold_", number)]] <- real_data  
  
  for (name in unique(df$name)) {
    
    small_data = df[df['name'] == name ,]
    
    threshold = dim(small_data)[1]/number
    
    list_values_row <- c(small_data[1,][, 0:6])
    
    for (residue in myList_save) {
    sum_value = sum(small_data[residue])
    
    if (sum_value >= threshold) {
      list_values_row<-append(list_values_row, as.integer(1))
    } else {
      list_values_row<- append(list_values_row, as.integer(0))
    }
    }
    
    empty_dict[[paste0("df_threshold_", number)]][nrow(empty_dict
                            [[paste0("df_threshold_", number)]]) + 1,] <- list_values_row
  }
  
    for (column_p in list_important_res) {
      new_col <- c()
    
      for (index in rownames(empty_dict[[paste0("df_threshold_", number)]])) {
        sum_res = sum(empty_dict[[paste0("df_threshold_", number)]][index, column_p])
        
        if (sum_res > 0) {
          new_col<-append(new_col, as.integer(1))
        } else {
          new_col<- append(new_col, as.integer(0))
        }
      }
      
      name_Y =paste0("residue", strsplit(column_p[1], split = '[.]')[[1]])
      empty_dict[[paste0("df_threshold_", number)]][name_Y[2]] <- new_col
    }
}

```
Eventually, we got 15 new dataframes describing each threshold. We put all of them into the dictionary empty_dict to use later. 

Besides the interaction fingerprint we have a formal charge feature. So, Let's have a look how formal charge can influence on the Permeability.coefficient. 
```{r fig.height = 3, fig.width = 5}
ggplot(empty_dict$df_threshold_6, aes(x = as.character(FormalCharge) ,
                                      y = Permeability.coefficient)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(x = "Charge", y = "Permeability.coefficient", 
  title = "Charge influence on permeability coefficient")
```
It is quiet obvious that charge is important property to separate the data points, and positive charged molecules have higher permeability coefficient.

After implementing the voting algorithm, our focus shifts to examining the simultaneous influence of charge and an essential residue (ASP113) on the Permeability coefficient. The plot below illustrates the results for a threshold of 6. Notably, positively charged compounds that interact with ASP113 display significantly higher Permeability coefficients.
It is worth to mention that graphs for all thresholds were build, but results are presented on selected threshold, which gives more insights about the data.
```{r fig.height = 4, fig.width = 8}
ggplot(empty_dict$df_threshold_6, aes(x=as.character(FormalCharge), 
    y=Permeability.coefficient, fill=as.character(residue113))) + 
    geom_boxplot() +
    facet_wrap(~FormalCharge, scale="free") +
    labs(x = "Charge + ASP113", y = "Permeability.coefficient", 
    title = "Influence of ASP113 on permeability coefficient for different charges")
```
We were curious about whether having many different interactions is important or if only five crucial ones would be enough to describe the data. To find out, we created a plot to see how the number of interactions affects the permeability coefficient. We also did the same analysis for only important residues and their impact on the permeability coefficient.

```{r echo=TRUE, fig.height=6, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
graths = c()
#all residues
for (number in threshold_list) {
  number_interaction <- c()
  for (index in rownames(empty_dict[[paste0("df_threshold_", number)]])) {
        sum_res = sum(empty_dict[[paste0("df_threshold_", number)]]
              [index, 9:(dim(empty_dict[[paste0("df_threshold_", number)]])[2] - 5 )])
        number_interaction  <- append(number_interaction, sum_res)
  }
  empty_dict[[paste0("df_threshold_", number)]]['all_interactions'] <- number_interaction
}

selected_threshold_list = c(15)
for (number in selected_threshold_list) {
plot = ggplot(empty_dict[[paste0("df_threshold_", number)]], 
  aes(x = Permeability.coefficient, y = all_interactions, 
  label = empty_dict[[paste0("df_threshold_", number)]]$Accumulation_class, 
  col = Accumulation_class)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5) +
  labs(x = "Permeability.coefficient", 
       y = "Number of all possible interactions", title = "All residues")
graths[['all_int']] = plot 
}

# five important residues
list_pp  <- c("residue113","residue117", "residue132", "residue82", "residue16")
for (number in threshold_list) {
  number_interaction <- c()
  for (index in rownames(empty_dict[[paste0("df_threshold_", number)]])) {
        sum_res = sum(empty_dict[[paste0("df_threshold_", number)]][index, list_pp])
        number_interaction  <- append(number_interaction, sum_res)
  }
  empty_dict[[paste0("df_threshold_", number)]]['all_important_residues'] <- 
    number_interaction
}
for (number in selected_threshold_list) {
plot_imp = ggplot(empty_dict[[paste0("df_threshold_", number)]],
  aes(x = Permeability.coefficient, y = all_important_residues,
  label = empty_dict[[paste0("df_threshold_", number)]]$Accumulation_class, 
  col = Accumulation_class)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5) +
  labs(x = "Permeability.coefficient", 
       y = "Number of interactions with important residues", title = "Selected residues")
graths[['all_int_imp']] = plot_imp
}

graths_plot <- grid.arrange(grobs=graths,ncol=2, nrow = 1)
```
From the plots we obtained for the threshold of 15, we can make the conclusion that the presence of five important residues has a greater influence on the permeability coefficient. Therefore, having many interactions does not seem to make much sense in this context.

#FROM HERE NEED TO READ ONCE AGAIN !!!

After applying "Voting algorithm" many features from interaction fingerprint might have identical values, which once again results in their insignificance for data point separation. Number of these features for each threshold is different, depending on how strict the threshold was. So, the code below is cleaning created data set for each threshold, saving only features that have different values, and might be useful for building a model.

```{r}
for (number in threshold_list) {
  list_residues = names(empty_dict[[paste0("df_threshold_", number)]][, 7:182])
  myList_delete <- c()
  myList_save <- c()
  
  for (name in list_residues) {
    sum_value = sum(empty_dict[[paste0("df_threshold_", number)]][name])
    if (sum_value == 0) {
      myList_delete <- append(myList_delete, name)
    }
    else {
      myList_save <- append(myList_save, name)
    }
  }
  empty_dict[[paste0("df_threshold_", number)]] = empty_dict[[paste0
        ("df_threshold_", number)]][,!(names(empty_dict[[paste0("df_threshold_", number)]]) 
                                       %in% myList_delete)]
  print(glue('Foe a dataframe with threshold {number} {length(myList_save)} features were 
       saved and {length(myList_delete)} features were deleted'))
}  
```
Now, when all dataframes do not have NaN values or features containing same values, we will prepare them for dimentionallity reduction analysis and further model building.
First it is essential to scale the "Formalcharge" property to have it in a range from 0 to 1. Interaction fingerprint string is already presented by binary notation with zeros and ones. 
Then, several properties created for plots building were deleted, and eventually our data is ready to work with.
```{r}
delete_list <- c( "residue113", "residue117", "residue132", "residue82", "residue16")
for (number in threshold_list) {
  
  empty_dict[[paste0("df_threshold_", number)]]['FormalCharge'] <- 
    predict(preProcess(empty_dict[[paste0("df_threshold_", number)]]['FormalCharge'], 
    method=c("range")), empty_dict[[paste0("df_threshold_", number)]]['FormalCharge'])
  
  empty_dict[[paste0("df_threshold_", number)]] = empty_dict[[paste0
        ("df_threshold_", number)]][,!(names(empty_dict[[paste0("df_threshold_", number)]])
                                       %in% delete_list)]
}
```
# PCA

In this project we used PCA to understand which threshold makes our data points more separable. It seems that the threshold 1 is to strict, and make a lot of dublicates, which will lead to mistakes in predictions. The threshold 15 at least separate the different compound. 

```{r fig.height=4, fig.width=10, warning=FALSE}
to_remove <- c("all_interactions", "all_important_residues", "ASP113_types_int")

threshold_list_pca = c(1, 15)
graths_pca = c()
for (number in threshold_list_pca) {
  empty_dict[[paste0("df_threshold_", number)]] = empty_dict[[paste0
    ("df_threshold_", number)]][ , !(names(empty_dict[[paste0
    ("df_threshold_", number)]]) %in% to_remove)]
  prcomp_df_threshold_3 <- prcomp(empty_dict[[paste0
    ("df_threshold_", number)]][, 7: dim(empty_dict[[paste0
    ("df_threshold_", number)]])[2]])
  pca_df_threshold_3 <- data.frame(
    PC1 = prcomp_df_threshold_3$x[, 1],
    PC2 = prcomp_df_threshold_3$x[, 2],
    classification = empty_dict[[paste0("df_threshold_", number)]]$Accumulation_class,
    label = empty_dict[[paste0("df_threshold_", number)]]$Accumulation_class
  )
  plot = ggplot(pca_df_threshold_3, aes(x = PC1, y = PC2, label = label, 
                                        col = classification)) +
    geom_point() +
    ggrepel::geom_text_repel(cex = 2.5)+
    labs(x = "PC1", y = "PC2", title = number)
  graths_pca[[glue("threshold_{number}")]] = plot
}
graths_plot_pca <- grid.arrange(grobs=graths_pca,ncol=2, nrow = 1)
```

Я думаю может не будем tSNE , PCA вроде хорошо получилось?

```{r}
# Preform TSNE on our data

rtsne_chololate_num <- Rtsne(empty_dict$df_threshold_3[, 7: dim(empty_dict$df_threshold_3)[2]],
  pca = FALSE, perplexity = 10,
  theta = 0.0, check_duplicates = FALSE
)

tsne_chololate_num <- data.frame(
  TSNE1 = rtsne_chololate_num$Y[, 1],
  TSNE2 = rtsne_chololate_num$Y[, 2],
  classification = empty_dict$df_threshold_3$Accumulation_class,
  label = empty_dict$df_threshold_3$Accumulation_class
)

ggplot(tsne_chololate_num, aes(
  x = TSNE1, y = TSNE2,
  label = label, col = classification
)) +
  geom_point() +
  ggrepel::geom_text_repel(cex = 2.5)
```
#Preparing data for model building
After performing different types or analysis and data preparation, it is now time to build model. Before that we need to make some adjustments for our target value column. Instead of having column with "High" and "Low' values, it is essential to switch to the binary representation with zeros and ones. 
Also the data is imbalanced which is illustrated below. It will be taken into the account for splitting the data into training and test sets.  
```{r}
for (number in threshold_list) {
  high_low <- c()
  for (index in rownames(empty_dict[[paste0("df_threshold_", number)]])) {
        high_low_value = empty_dict[[paste0("df_threshold_", number)]][index,
                                                          "Accumulation_class"]
        if (high_low_value == 'High') {
          high_low = append(high_low, as.character(1))
        } else {
          high_low = append(high_low, as.character(0))
        }
  }
  empty_dict[[paste0("df_threshold_", number)]]['High_Low_value'] <- high_low
}
empty_dict$df_threshold_3 %>% 
  count(High_Low_value) %>% 
  mutate(prop = n/sum(n))
```
For building classification model four main algorithms were chosen: Random Forest, Decision tree, Support vector machine and Kneibours. Several dictionaries were created to store the obtained results.   
```{r}
# types of models to use
classifiers_to_use = c('Random Forest', 'Decition tree', 'SVM', 'KNN')
#results dictionary 
models_dict <- list()
# confusion matrix dictionary
conf_matrix <- list()
# balanced accuracy dictionary
accuracy_matrix <- list()

model_data = data.frame(matrix(ncol = 3, nrow = 0))
colnames(model_data) <- c("Threshold", "Accuracy" , "Balanced_Accuracy")
for (classifier in classifiers_to_use) {
  
  models_dict[[classifier]] <- model_data
  conf_matrix[[classifier]] <- model_data
  accuracy_matrix[[classifier]] <- 0
}
```
The script below describes building 4 models discussed above for each threshold (15 all together), so 60 models. At this stage there are no parametr's tuning, 
The goal is to compare which threshold is better for different types of model. 

```{r message=FALSE, warning=FALSE}
set.seed(123)
for (number in threshold_list) {
  #data split  
  split_threshold <- initial_split(empty_dict[[paste0("df_threshold_", number)]] %>% 
        select(-Canonical.SMILES, -Ligand, -Permeability.coefficient, -name, -Accumulation_class),
        strata=as.integer(empty_dict[[paste0("df_threshold_", number)]]$High_Low_value), prop = 7/10)
  
  split_threshold_train <- training(split_threshold)
  split_threshold_test  <- testing(split_threshold)
  
  split_threshold_train$High_Low_value  <- as.factor(split_threshold_train$High_Low_value)
  split_threshold_test$High_Low_value <- as.factor(split_threshold_test$High_Low_value)
  
  # WHAT MODEL TO USE?
  for (name_cl in classifiers_to_use) {
    
    if (name_cl == "Random Forest") {
      rf_mod = rand_forest(trees = 1000) %>% set_engine("ranger") %>% set_mode("classification")
    }
    if (name_cl == "Decition tree") {
      rf_mod = decision_tree() %>% set_engine("rpart") %>% set_mode("classification")
    }
    if (name_cl == "SVM") {
      rf_fit = svm(formula = High_Low_value ~ .,
                 data = split_threshold_train,
                 type = 'C-classification',
                 kernel = "linear")
    }
    if (name_cl == "KNN") {
      rf_fit = knn(train = split_threshold_train,
                      test = split_threshold_test,
                      cl = split_threshold_train$High_Low_value,
                      k = 3)
    }
    #fitting model
    if (name_cl == 'Random Forest' | name_cl == 'Decition tree') {
      rf_fit <- 
      rf_mod %>% 
      fit(High_Low_value ~ ., data = split_threshold_train)
    }
    if (name_cl == 'Random Forest' | name_cl == 'Decition tree' | name_cl == 'SVM') {
      rf_testing_pred <- 
      predict(rf_fit, split_threshold_test[-dim(split_threshold_test)[2]]) %>% 
        bind_cols(predict(rf_fit, 
          split_threshold_test[-dim(split_threshold_test)[2]], type = "prob")) %>% 
        bind_cols(split_threshold_test %>% select(High_Low_value))
    } else {
      rf_testing_pred <- rf_fit %>%
        bind_cols(split_threshold_test %>% select(High_Low_value))
    }
    # Accuracy calculation
    acc = rf_testing_pred %>%                   # test set predictions
      accuracy(truth =High_Low_value, colnames(rf_testing_pred[1]))
    
    baL_acc = rf_testing_pred %>%                   # test set predictions
    bal_accuracy(truth =High_Low_value, colnames(rf_testing_pred[1]))
    
    models_dict[[name_cl]][nrow(models_dict[[name_cl]]) + 1,] <- 
      c(number, acc %>% pull(.estimate), baL_acc %>% pull(.estimate))
    
    #confusion matrix
    if (baL_acc %>% pull(.estimate) > accuracy_matrix[[name_cl]]) {
      conf_matrix[[name_cl]] <- confusionMatrix(data=rf_testing_pred
        [[colnames(rf_testing_pred[1])]], reference = rf_testing_pred$High_Low_value)
      accuracy_matrix[[name_cl]] = baL_acc %>% pull(.estimate)
    }
  }
}
```
By building plot below eventually we can conclude that ...

STOPPED HERE

```{r echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
plots = c()
for (name_cl in classifiers_to_use) {
hgh = models_dict[[name_cl]]
oo =ggplot(hgh, aes(x=Threshold, y=Balanced_Accuracy)) +
  geom_point()+
  geom_line() +
  xlab("Thresholds") + ylab("Balanced accuracy") +
  ylim(0.4, 1.0 )+
  stat_smooth()+
  ggtitle(name_cl)
plots[[name_cl]] = oo 
}
grid.arrange(grobs=plots,ncol=2, nrow = 2)
```
Tuning

```{r}
fitControl <- trainControl(method = "repeatedcv", 
                           repeats = 1)
```


```{r warning=FALSE}
#set.seed(123)
cv_model= data.frame(matrix(ncol = 2, nrow = 0))
colnames(cv_model) <- c("Threshold" , "Accuracy")

set.seed(10)
for (number in threshold_list) {
  
  svmModel <- train(High_Low_value ~ ., data = empty_dict[[paste0
    ("df_threshold_", number)]] %>% select(-Canonical.SMILES, -Ligand,
    -Permeability.coefficient, -name, -Accumulation_class),
                  method = "rpart", 
                  trControl = fitControl , 
                  metric = 'Accuracy')
  pp = svmModel$resample
  svmAccuracy <-mean(pp$Accuracy)
  
  cv_model[nrow(cv_model) + 1,] <- c(number, svmAccuracy)
}
ggplot(cv_model, aes(x=Threshold, y=Accuracy)) +
  geom_point()+
  geom_line() +
  xlab("Thresholds") + ylab("Accuracy") +
  ylim(0.7, 0.95 )+
  stat_smooth()+
  ggtitle('Decision tree: cross fold validation')

```
tuning 

```{r}
  # defining the parameters for the decision tree
  tune_spec <- 
    decision_tree(
      cost_complexity = tune(), # to control the size of the tree
      tree_depth = tune()
    ) %>% 
    set_engine("rpart") %>% 
    set_mode("classification")
  
  tree_grid <- grid_regular(cost_complexity(), 
                            tree_depth(),
                            levels = 5)
  
  set.seed(234)
  empty_dict$df_threshold_13$High_Low_value<- as.factor(
    empty_dict$df_threshold_13$High_Low_value)
  thr_folds <- vfold_cv(empty_dict$df_threshold_13 %>% select(-Canonical.SMILES, 
                -Ligand, -Permeability.coefficient, -name, -Accumulation_class))
  
  
  # define the workflow
  tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(High_Low_value ~ .)
  
  tree_res <- 
    tree_wf %>% 
    tune_grid(
      resamples = thr_folds,
      grid = tree_grid
      )
    
  # plot preformance
tree_res %>%
    collect_metrics() %>%
    mutate(tree_depth = factor(tree_depth)) %>%
    ggplot(aes(cost_complexity, mean, color = tree_depth)) +
    geom_line(size = 1.5, alpha = 0.6) +
    geom_point(size = 2) +
    facet_wrap(~ .metric, scales = "free", nrow = 2) +
    scale_x_log10(labels = scales::label_number()) +
    scale_color_viridis_d(option = "plasma", begin = .9, end = 0)+
    labs(title = '13')

```

```{r}
best_tree  = tree_res %>%
  show_best("accuracy")
best_tree[1, ]
```

```{r}
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree[1, ])

final_wf
```

```{r}

#data split  

empty_dict$df_threshold_13$High_Low_value = as.character(empty_dict$df_threshold_13$High_Low_value)
empty_dict$df_threshold_13$High_Low_value_dublicate = empty_dict$df_threshold_13$High_Low_value

empty_dict$df_threshold_13$High_Low_value = as.factor(empty_dict$df_threshold_13$High_Low_value)




split_threshold <- initial_split(empty_dict$df_threshold_13 %>% select(-Canonical.SMILES, -Ligand, 
      -Permeability.coefficient, -name, -Accumulation_class, -High_Low_value_dublicate), prop = 7/10, 
      strata=as.integer(empty_dict$df_threshold_13$High_Low_value_dublicate))

    
split_threshold_train <- training(split_threshold)
split_threshold_test  <- testing(split_threshold)

split_threshold$High_Low_value  <- as.factor(split_threshold$High_Low_value)
    
#split_threshold_train$High_Low_value  <- as.factor(split_threshold_train$High_Low_value)
#split_threshold_test$High_Low_value <- as.factor(split_threshold_test$High_Low_value)

final_tree <- 
  final_wf %>%
  fit(data = split_threshold_train) 

final_fit <- 
  final_wf %>%
  last_fit(split_threshold) 
  
```

```{r}
final_fit %>%
  collect_predictions()
```


```{r}
# collect metrics
final_fit %>%
  collect_predictions() %>% 
  accuracy(truth =High_Low_value, .pred_class)

```
```{r}
confusionMatrix(data=kk$.pred_class, reference = kk$High_Low_value)

```






